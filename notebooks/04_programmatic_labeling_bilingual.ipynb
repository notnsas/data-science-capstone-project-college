{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOmdf15fW+Bx+7ZB2uuxabr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LatiefDataVisionary/data-science-capstone-project-college/blob/main/notebooks/04_programmatic_labeling_bilingual.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1efa4a5d"
      },
      "source": [
        "# **04 Programmatic Sentiment Labeling (Bilingual)**\n",
        "\n",
        "This notebook performs programmatic sentiment labeling on a clean, unlabeled, bilingual (English & Indonesian) dataset. It employs a hybrid, language-specific approach:\n",
        "\n",
        "- For English reviews, it utilizes the fast, lexicon-based VADER tool.\n",
        "- For Indonesian reviews, it leverages a powerful, pre-trained Transformer-based model fine-tuned for Indonesian sentiment.\n",
        "\n",
        "The input is the cleaned dataset, and the output is a single, consistently labeled dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ebc870c"
      },
      "source": [
        "## **1. Setup and Data Loading**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "021490ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bac0d53-fc62-4a83-877c-90e41698a44c"
      },
      "source": [
        "# Install necessary libraries\n",
        "!pip install langdetect transformers[torch] tqdm\n",
        "!pip install accelerate -qqq # Required by some Hugging Face models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m972.8/981.5 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.6.2)\n",
            "Requirement already satisfied: torch>=2.2 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2.8.0+cu126)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (1.10.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.2->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.2->transformers[torch]) (3.0.3)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=b31035e9fa342995ca8d7858baf73d9a14eba98693e7895c1ae36dc4453f1dc8\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1833787"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers.pipelines import pipeline\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from langdetect import detect, DetectorFactory\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Ensure consistent language detection results\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "tqdm.pandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31fb2ea3"
      },
      "source": [
        "# Download the VADER lexicon\n",
        "try:\n",
        "    nltk.data.find('sentiment/vader_lexicon.zip')\n",
        "except Exception: # Catching generic Exception as DownloadError might not be directly accessible\n",
        "    nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzzxiQrjN298"
      },
      "outputs": [],
      "source": [
        "# reviews_cleaned_en = 'https://raw.githubusercontent.com/LatiefDataVisionary/data-science-capstone-project-college/refs/heads/main/data/processed/reviews_cleaned_en.csv'\n",
        "# reviews_cleaned_id = 'https://raw.githubusercontent.com/LatiefDataVisionary/data-science-capstone-project-college/refs/heads/main/data/processed/reviews_cleaned_id.csv'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_cleaned_en = 'https://raw.githubusercontent.com/LatiefDataVisionary/data-science-capstone-project-college/refs/heads/main/data/processed/reviews_cleaned_en_tokenized.csv'\n",
        "reviews_cleaned_id = 'https://raw.githubusercontent.com/LatiefDataVisionary/data-science-capstone-project-college/refs/heads/main/data/processed/reviews_cleaned_id_tokenized.csv'"
      ],
      "metadata": {
        "id": "7h5OWAf0oX7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1994c3eb"
      },
      "source": [
        "# Load the cleaned dataset\n",
        "# Note: Assuming a single combined CSV after cleaning, based on the prompt.\n",
        "# If separate files exist, they should be loaded and combined here.\n",
        "# The prompt mentions '../data/processed/reviews_cleaned.csv' as the input.\n",
        "# However, the user's initial cells define reviews_cleaned_en and reviews_cleaned_id URLs.\n",
        "# For this notebook, I will assume the input is a single combined file as per the prompt.\n",
        "# If the user intended to use the separate files, this cell would need modification.\n",
        "try:\n",
        "    df = pd.read_csv('../data/processed/reviews_cleaned.csv')\n",
        "except FileNotFoundError:\n",
        "    # Fallback: If the combined file isn't found, load from URLs and combine\n",
        "    print(\"reviews_cleaned.csv not found. Attempting to load from provided URLs.\")\n",
        "    reviews_cleaned_en = 'https://raw.githubusercontent.com/LatiefDataVisionary/data-science-capstone-project-college/refs/heads/main/data/processed/reviews_cleaned_en.csv'\n",
        "    reviews_cleaned_id = 'https://raw.githubusercontent.com/LatiefDataVisionary/data-science-capstone-project-college/refs/heads/main/data/processed/reviews_cleaned_id.csv'\n",
        "\n",
        "    df_en = pd.read_csv(reviews_cleaned_en)\n",
        "    df_id = pd.read_csv(reviews_cleaned_id)\n",
        "\n",
        "    # Assuming the dataframes have a 'review_text' column and similar structure\n",
        "    df = pd.concat([df_en, df_id], ignore_index=True)\n",
        "    print(\"Data loaded and combined from URLs.\")\n",
        "\n",
        "display(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed509f59"
      },
      "source": [
        "## 2. Language Detection: The Core Logic Gate\n",
        "\n",
        "The first crucial step is to reliably detect the language of each review. This will act as a switch to direct the review to the correct labeling pipeline, ensuring that English reviews are processed by VADER and Indonesian reviews by the Transformer model. A robust language detection library is essential for this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3f878fa"
      },
      "source": [
        "# Function to detect language with error handling\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        # Langdetect can struggle with very short texts\n",
        "        if pd.isna(text) or len(str(text).strip()) < 5:\n",
        "            return 'unknown'\n",
        "        return detect(str(text))\n",
        "    except:\n",
        "        return 'unknown' # Handle potential errors during detection\n",
        "\n",
        "# Apply language detection\n",
        "df['language'] = df['content'].progress_apply(detect_language)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "863e08eb"
      },
      "source": [
        "# Show the distribution of detected languages\n",
        "display(df['language'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3b9e1de"
      },
      "source": [
        "## 3. Pipeline A - Labeling English Reviews with VADER\n",
        "\n",
        "For English reviews, we will use VADER (Valence Aware Dictionary and sEntiment Reasoner). VADER is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media, and it is known for being fast and effective for English text. It provides a compound score ranging from -1 (most negative) to +1 (most positive)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c82966e1"
      },
      "source": [
        "# Initialize the VADER sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b89223e"
      },
      "source": [
        "# Function to label English sentiment using VADER compound score\n",
        "def label_english_sentiment(text):\n",
        "    if pd.isna(text):\n",
        "        return None\n",
        "    scores = sia.polarity_scores(str(text))\n",
        "    compound_score = scores['compound']\n",
        "    if compound_score >= 0.05:\n",
        "        return 'positive'\n",
        "    elif compound_score <= -0.05:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d5c6023"
      },
      "source": [
        "# Apply VADER labeling to English reviews\n",
        "# Initialize sentiment_label column to None\n",
        "df['sentiment_label'] = None\n",
        "\n",
        "# Apply the labeling function only to English reviews\n",
        "english_mask = (df['language'] == 'en')\n",
        "df.loc[english_mask, 'sentiment_label'] = df.loc[english_mask, 'content'].progress_apply(label_english_sentiment)\n",
        "\n",
        "# Check how many English reviews were labeled\n",
        "print(f\"Number of English reviews labeled: {df[english_mask]['sentiment_label'].notna().sum()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "621ab17e"
      },
      "source": [
        "## 4. Pipeline B - Labeling Indonesian Reviews with a Transformer Model\n",
        "\n",
        "For Indonesian reviews, a more sophisticated, context-aware model is needed as VADER is not designed for this language. We will use a pre-trained Transformer model that has been fine-tuned for Indonesian sentiment analysis. This type of model captures nuances in language more effectively than lexicon-based methods for complex languages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a25fc62f"
      },
      "source": [
        "# Ambil token dari Colab Secrets\n",
        "# Pastikan nama secret-nya adalah 'HF_TOKEN'\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Lakukan login\n",
        "login(token=hf_token)\n",
        "\n",
        "print(\"Login Hugging Face berhasil!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bdae559"
      },
      "source": [
        "# Define the model checkpoint for an Indonesian sentiment model\n",
        "# Using \"w11wo/indonesian-roberta-base-sentiment-classifier\" with pipeline.\n",
        "# If you encounter issues loading the model, ensure you are authenticated with Hugging Face\n",
        "# (e.g., by setting a HF_TOKEN in Colab secrets) or try a different public model.\n",
        "model_checkpoint = \"w11wo/indonesian-roberta-base-sentiment-classifier\"\n",
        "\n",
        "# Load the sentiment analysis pipeline\n",
        "try:\n",
        "    sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model_checkpoint)\n",
        "    print(f\"Pipeline loaded successfully using model: {model_checkpoint}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading pipeline with model {model_checkpoint}: {e}\")\n",
        "    sentiment_pipeline = None # Set to None if loading fails\n",
        "\n",
        "# Define a simple mapping for labels if needed (check model card for exact labels)\n",
        "# This model typically outputs 'LABEL_0', 'LABEL_1', 'LABEL_2'\n",
        "# You might need to inspect the model output or model card to map these to 'negative', 'neutral', 'positive'\n",
        "# For now, let's assume a common mapping or inspect the output later.\n",
        "# Based on the model card, LABEL_0=negative, LABEL_1=neutral, LABEL_2=positive\n",
        "id_label_mapping = {'LABEL_0': 'negative', 'LABEL_1': 'neutral', 'LABEL_2': 'positive'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5c5084d"
      },
      "source": [
        "### Tutorial: Menghubungkan Token Hugging Face ke Google Colab\n",
        "\n",
        "Untuk dapat mengakses model-model tertentu di Hugging Face dari Google Colab, terutama jika model tersebut memerlukan autentikasi, Anda perlu menghubungkan token akses Hugging Face Anda ke lingkungan Colab. Cara paling aman adalah dengan menggunakan fitur **Colab Secrets**.\n",
        "\n",
        "Berikut langkah-langkahnya:\n",
        "\n",
        "1.  **Dapatkan Token Akses dari Hugging Face:**\n",
        "    *   Buka website Hugging Face: [huggingface.co](https://huggingface.co/)\n",
        "    *   Login ke akun Hugging Face Anda. Jika belum punya, daftar terlebih dahulu.\n",
        "    *   Setelah login, klik **foto profil Anda** di kanan atas halaman.\n",
        "    *   Pilih **\"Settings\"** dari menu dropdown.\n",
        "    *   Di menu navigasi sebelah kiri pada halaman Settings, klik **\"Access Tokens\"**.\n",
        "    *   Klik tombol **\"New token\"** untuk membuat token baru.\n",
        "    *   Beri nama token Anda (misalnya, `colab-access`, `my-project-token`, dll.). Nama ini hanya untuk identifikasi di akun Hugging Face Anda.\n",
        "    *   Pilih peran (Role) token: Untuk memuat model, peran **\"read\"** sudah cukup. Jika Anda berencana mengunggah sesuatu, pilih \"write\".\n",
        "    *   Klik tombol **\"Generate token\"**.\n",
        "    *   Token akan muncul di layar. **Salin token ini segera** karena Anda tidak akan bisa melihatnya lagi nanti. Simpan di tempat yang aman sementara jika perlu, tapi jangan masukkan langsung ke kode notebook yang akan dibagikan.\n",
        "\n",
        "2.  **Simpan Token di Google Colab Secrets:**\n",
        "    *   Kembali ke Google Colab notebook Anda.\n",
        "    *   Di sidebar kiri Colab, temukan dan klik ikon **Kunci (üîí)**. Ini adalah panel Secrets.\n",
        "    *   Klik tombol **\"+ New secret\"**.\n",
        "    *   Pada kolom **\"Name\"**, masukkan nama secret Anda. **Sangat penting** untuk menggunakan nama yang akan Anda panggil di kode. Dalam kasus notebook ini, kita menggunakan nama **`HF_TOKEN`**. Jadi, ketik `HF_TOKEN`.\n",
        "    *   Pada kolom **\"Value\"**, tempel (paste) token akses Hugging Face yang baru saja Anda salin.\n",
        "    *   Pastikan tombol **\"Notebook access\"** diaktifkan (berwarna hijau atau tercentang). Ini memungkinkan notebook Anda mengakses secret ini.\n",
        "    *   (Opsional) Anda bisa menambahkan deskripsi untuk secret ini.\n",
        "    *   Setelah selesai, secret akan tersimpan secara otomatis.\n",
        "\n",
        "3.  **Gunakan Token di Kode Colab untuk Login:**\n",
        "    *   Sekarang, Anda bisa menggunakan kode Python di notebook Anda untuk mengambil secret `HF_TOKEN` dan melakukan login ke Hugging Face. Kode ini akan mengambil token dari Colab Secrets, bukan dari kode yang terlihat, sehingga lebih aman.\n",
        "\n",
        "Berikut kode Python yang perlu Anda jalankan di notebook Anda (ini adalah sel yang sebelumnya error `SecretNotFoundError`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16a20ced"
      },
      "source": [
        "# Function to label Indonesian sentiment using the Transformer model\n",
        "def label_indonesian_sentiment(text):\n",
        "    if pd.isna(text):\n",
        "        return None\n",
        "    try:\n",
        "        # Tokenize the text\n",
        "        inputs = tokenizer(str(text), return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "\n",
        "        # Pass the tokens through the model\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        # Get the logits and apply softmax to get probabilities\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.softmax(logits, dim=1)[0]\n",
        "\n",
        "        # Determine the predicted label (index with the highest probability)\n",
        "        predicted_class_id = probabilities.argmax().item()\n",
        "\n",
        "        # Return the corresponding label\n",
        "        return id_labels[predicted_class_id]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error labeling text: {text[:50]}... Error: {e}\")\n",
        "        return None # Return None if labeling fails"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb5a9295"
      },
      "source": [
        "# Apply Transformer labeling to Indonesian reviews that are not yet labeled\n",
        "indonesian_mask = (df['language'] == 'id') & (df['sentiment_label'].isna())\n",
        "df.loc[indonesian_mask, 'sentiment_label'] = df.loc[indonesian_mask, 'content'].progress_apply(label_indonesian_sentiment)\n",
        "\n",
        "# Check how many Indonesian reviews were labeled\n",
        "print(f\"Number of Indonesian reviews labeled: {df[indonesian_mask]['sentiment_label'].notna().sum()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7fbf1f0"
      },
      "source": [
        "## 5. Reviewing and Finalizing the Labeled Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9092d44"
      },
      "source": [
        "# Check for any reviews that were not labeled\n",
        "unlabeled_count = df['sentiment_label'].isnull().sum()\n",
        "print(f\"Number of reviews that were not labeled: {unlabeled_count}\")\n",
        "if unlabeled_count > 0:\n",
        "    print(\"Check the language detection results for these reviews, or if any errors occurred during labeling.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a213e59c"
      },
      "source": [
        "# Display the final distribution of the combined labels\n",
        "display(df['sentiment_label'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "037991f5"
      },
      "source": [
        "# Display a few examples of English reviews and their VADER-generated labels\n",
        "print(\"Examples of English reviews and their VADER labels:\")\n",
        "display(df[df['language'] == 'en'][['content', 'sentiment_label']].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b5a4c5b"
      },
      "source": [
        "# Display a few examples of Indonesian reviews and their Transformer-generated labels\n",
        "print(\"\\nExamples of Indonesian reviews and their Transformer labels:\")\n",
        "display(df[df['language'] == 'id'][['content', 'sentiment_label']].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15e16558"
      },
      "source": [
        "## 6. Saving the Labeled Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a48f37d"
      },
      "source": [
        "# Define the output directory and filenames\n",
        "output_dir = '../data/processed/'\n",
        "# output_path_en = os.path.join(output_dir, 'reviews_labeled_en.csv')\n",
        "# output_path_id = os.path.join(output_dir, 'reviews_labeled_id.csv')\n",
        "output_path_en = os.path.join(output_dir, 'reviews_labeled_en_tokenized.csv')\n",
        "output_path_id = os.path.join(output_dir, 'reviews_labeled_id_tokenized.csv')\n",
        "\n",
        "output_path_other = os.path.join(output_dir, 'reviews_labeled_other_languages.csv') # To save other languages\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"Output directory '{output_dir}' ensured to exist.\")\n",
        "\n",
        "# Filter the DataFrame by language\n",
        "df_en_labeled = df[df['language'] == 'en']\n",
        "df_id_labeled = df[df['language'] == 'id']\n",
        "df_other_languages = df[~df['language'].isin(['en', 'id'])]\n",
        "\n",
        "\n",
        "# Save the labeled DataFrames to separate CSV files\n",
        "try:\n",
        "    df_en_labeled.to_csv(output_path_en, index=False)\n",
        "    print(f\"Labeled English dataset saved to {output_path_en}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving English dataset: {e}\")\n",
        "\n",
        "try:\n",
        "    df_id_labeled.to_csv(output_path_id, index=False)\n",
        "    print(f\"Labeled Indonesian dataset saved to {output_path_id}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving Indonesian dataset: {e}\")\n",
        "\n",
        "try:\n",
        "    df_other_languages.to_csv(output_path_other, index=False)\n",
        "    print(f\"Labeled other languages dataset saved to {output_path_other}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving other languages dataset: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31603400"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook successfully implemented a hybrid, language-specific approach to sentiment labeling on a bilingual dataset. By first detecting the language of each review, we were able to apply the most appropriate tool for each language ‚Äì VADER for English and a Transformer model for Indonesian ‚Äì resulting in a consistently labeled dataset ready for further analysis or model training."
      ]
    }
  ]
}