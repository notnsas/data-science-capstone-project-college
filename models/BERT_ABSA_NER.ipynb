{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d803a609-270f-4b89-b853-ba857003b037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "else:\n",
    "    print(\"Please switch to a GPU-enabled environment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684232d5-94c1-48af-bef4-f4e0130c7043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Load data\n",
    "# df1 = pd.read_csv(\"https://raw.githubusercontent.com/LatiefDataVisionary/data-science-capstone-project-college/refs/heads/main/data/processed/reviews_cleaned_en.csv\")\n",
    "# df2 = pd.read_csv(\"https://raw.githubusercontent.com/LatiefDataVisionary/data-science-capstone-project-college/refs/heads/main/data/processed/reviews_cleaned_id.csv\")\n",
    "\n",
    "# # Combine the 'content' columns\n",
    "# combined_content = pd.concat([df1['content'], df2['content']], ignore_index=True)\n",
    "\n",
    "# # Make it a DataFrame\n",
    "# df = pd.DataFrame({'content': combined_content})\n",
    "\n",
    "# # Train-test split\n",
    "# train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# print(train_df.shape, test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b250402a-e695-4d0b-b186-33405cc932a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Save to CSV ---\n",
    "# train_df.to_csv(\"train_df.csv\", index=False)\n",
    "# test_df.to_csv(\"test_df.csv\", index=False)\n",
    "\n",
    "# # --- Load again ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c21f322-87f0-4133-be95-4a98eefb95d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# train_df = pd.read_csv(\"train_df.csv\")\n",
    "# test_df = pd.read_csv(\"test_df.csv\")\n",
    "\n",
    "# print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1d0e6f-d000-47cf-a158-a837d96eb131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df, test_df = train_df[:1000], test_df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6177142-81d9-49eb-9e42-34a43ac0bc31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# # --- Load pipelines with GPU ---\n",
    "# aspect_pipeline = pipeline(\n",
    "#     \"token-classification\",\n",
    "#     model=\"Sengil/bert-based-aspect-extraction\",\n",
    "#     aggregation_strategy=\"max\",\n",
    "#     device=0  # GPU\n",
    "# )\n",
    "\n",
    "# sentiment_pipeline = pipeline(\n",
    "#     \"text-classification\",\n",
    "#     model=\"yangheng/deberta-v3-large-absa-v1.1\",\n",
    "#     device=0  # GPU\n",
    "# )\n",
    "\n",
    "# # --- Batch extraction function ---\n",
    "# def extract_aspects_batch(texts, batch_size=32):\n",
    "#     all_annotations = []\n",
    "#     for i in range(0, len(texts), batch_size):\n",
    "#         if (i % 20) == 0:\n",
    "#             print(f'batch ke {i/batch_size}')\n",
    "#         batch_texts = texts[i:i+batch_size]\n",
    "#         # Extract aspects for the batch\n",
    "#         # print(f'batch_texts : {batch_texts}')\n",
    "#         batch_aspects = aspect_pipeline(batch_texts)\n",
    "#         batch_annotations = []\n",
    "\n",
    "#         for j, aspects in enumerate(batch_aspects):\n",
    "#             text = batch_texts[j]\n",
    "#             annotations = []\n",
    "#             for r in aspects:\n",
    "#                 term = r[\"word\"]\n",
    "#                 start = r[\"start\"]\n",
    "#                 end = r[\"end\"]\n",
    "\n",
    "#                 # Sentiment for each aspect in batch\n",
    "#                 sentiment = sentiment_pipeline(term, text_pair=text)[0]\n",
    "#                 annotations.append({\n",
    "#                     \"term\": term,\n",
    "#                     \"from\": start,\n",
    "#                     \"to\": end,\n",
    "#                     \"sentiment\": sentiment[\"label\"],\n",
    "#                     \"score\": sentiment[\"score\"]\n",
    "#                 })\n",
    "#             text = text.split()\n",
    "#             batch_annotations.append({\"token\": text, \"aspects\": annotations})\n",
    "#         all_annotations.extend(batch_annotations)\n",
    "#     return all_annotations\n",
    "\n",
    "# # --- Train annotations ---\n",
    "# train_df[\"annotations\"] = extract_aspects_batch(train_df['content'].tolist())\n",
    "# train_df[\"annotations\"].to_json(\"absa_train.json\", orient=\"records\", indent=2, force_ascii=False)\n",
    "\n",
    "# # --- Test annotations ---\n",
    "# test_df[\"annotations\"] = extract_aspects_batch(test_df['content'].tolist())\n",
    "# test_df[\"annotations\"].to_json(\"absa_test.json\", orient=\"records\", indent=2, force_ascii=False)\n",
    "\n",
    "# print(\"‚úÖ Done! Train and test annotations saved as JSON separately.\")\n",
    "\n",
    "\n",
    "# # --- Convert to dataframe ---\n",
    "# # records = []\n",
    "# # for ann in annotations:\n",
    "# #     text = ann[\"text\"]\n",
    "# #     for asp in ann[\"aspects\"]:\n",
    "# #         records.append({\n",
    "# #             \"text\": text,\n",
    "# #             \"aspect\": asp[\"term\"],\n",
    "# #             \"from\": asp[\"from\"],\n",
    "# #             \"to\": asp[\"to\"],\n",
    "# #             \"sentiment\": asp[\"sentiment\"],\n",
    "# #             \"score\": asp[\"score\"]\n",
    "# #         })\n",
    "\n",
    "# # absa_df = pd.DataFrame(records)\n",
    "# # absa_df.to_csv(\"absa_model_output.csv\", index=False)\n",
    "# print(\"‚úÖ Done! Batch ABSA extraction + sentiment prediction completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7db926-13cd-450d-ac64-c89a9bd01646",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# import math\n",
    "# import json\n",
    "\n",
    "# # --- Load pipelines with GPU ---\n",
    "# aspect_pipeline = pipeline(\n",
    "#     \"token-classification\",\n",
    "#     model=\"Sengil/bert-based-aspect-extraction\",\n",
    "#     aggregation_strategy=\"max\",\n",
    "#     device=0  # GPU\n",
    "# )\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# train_df = train_df.dropna()\n",
    "# test_df = test_df.dropna()\n",
    "# # --- Sentiment model ---\n",
    "# sent_model_name = \"yangheng/deberta-v3-base-absa-v1.1\"  # base version to fit GPU\n",
    "# sent_tokenizer = AutoTokenizer.from_pretrained(sent_model_name)\n",
    "# sent_model = AutoModelForSequenceClassification.from_pretrained(sent_model_name).to(device)\n",
    "# sent_model.eval()\n",
    "\n",
    "# # --- Example batch ---\n",
    "# batch_texts = [\n",
    "#     \"The food was great but the service was slow.\",\n",
    "#     \"The battery life of this phone is terrible.\"\n",
    "# ]\n",
    "\n",
    "# # --- Batch extraction function ---\n",
    "# def extract_aspects_batch(texts, batch_size=16):\n",
    "#     all_annotations = []\n",
    "#     for i in range(0, len(texts), batch_size):\n",
    "#         if (i % 20) == 0:\n",
    "#             print(f'batch ke {i/batch_size}')\n",
    "#         batch_texts = texts[i:i+batch_size]\n",
    "#         # Extract aspects for the batch\n",
    "#         # print(f'batch_texts : {batch_texts}')\n",
    "#         try:\n",
    "#             batch_aspects = aspect_pipeline(batch_texts)\n",
    "        \n",
    "\n",
    "#             # --- Prepare batch for sentiment ---\n",
    "#             sent_texts = []\n",
    "#             sent_aspects = []\n",
    "#             aspect_map = []  # to map back to original text/aspect\n",
    "            \n",
    "#             for i, (text, aspects) in enumerate(zip(batch_texts, batch_aspects)):\n",
    "#                 for aspect in aspects:\n",
    "#                     term = aspect[\"word\"]\n",
    "#                     sent_texts.append(text)\n",
    "#                     sent_aspects.append(term)\n",
    "#                     aspect_map.append(i)  # keep track of which text this aspect belongs to\n",
    "            \n",
    "#             # --- Tokenize for sentiment in batch ---\n",
    "#             sent_inputs = sent_tokenizer(sent_texts, text_pair=sent_aspects, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "#             # --- Forward pass sentiment model ---\n",
    "#             with torch.no_grad():\n",
    "#                 outputs = sent_model(**sent_inputs)\n",
    "#                 probs = torch.softmax(outputs.logits, dim=-1)\n",
    "#                 preds = torch.argmax(probs, dim=-1)\n",
    "            \n",
    "#            # --- Initialize batch_annotations for this batch ---\n",
    "#             # batch_annotations = []\n",
    "            \n",
    "#             batch_annotations = [[] for _ in batch_texts]\n",
    "    \n",
    "#             for idx_in_flattened, text_idx in enumerate(aspect_map):\n",
    "#                 batch_annotations[text_idx].append({\n",
    "#                     \"term\": sent_aspects[idx_in_flattened],\n",
    "#                     \"sentiment\": sent_model.config.id2label[preds[idx_in_flattened].item()],\n",
    "#                     \"score\": probs[idx_in_flattened][preds[idx_in_flattened]].item()\n",
    "#                 })\n",
    "            \n",
    "#             # Then add tokenized text\n",
    "#             for i, ann in enumerate(batch_annotations):\n",
    "#                 batch_annotations[i] = {\"token\": batch_texts[i].split(), \"aspects\": ann}\n",
    "#             all_annotations.extend(batch_annotations)\n",
    "#             # batch_annotations.append({\n",
    "#             #     \"token\": text,\n",
    "#             #     \"aspects\": ann\n",
    "#             # })\n",
    "#             torch.cuda.empty_cache()\n",
    "#             del sent_inputs, outputs, probs, preds\n",
    "#         except:\n",
    "#             continue\n",
    "        \n",
    "#     return all_annotations\n",
    "\n",
    "# # --- Helper to process and save in chunks ---\n",
    "# def process_and_save(df, col_name, output_json, chunk_size=16000):\n",
    "#     all_annotations = []\n",
    "#     total = len(df)\n",
    "#     num_chunks = math.ceil(total / chunk_size)\n",
    "\n",
    "#     for i in range(6, num_chunks):\n",
    "#         start = i * chunk_size\n",
    "#         end = min(start + chunk_size, total)\n",
    "#         print(f\"Processing chunk {i+1}/{num_chunks} ({start}-{end})\")\n",
    "\n",
    "#         token_val = [str(t) for t in df[col_name].iloc[start:end].values]\n",
    "#         chunk_annotations = extract_aspects_batch(token_val)\n",
    "#         all_annotations.extend(chunk_annotations)\n",
    "\n",
    "#         # Save after each chunk to avoid losing progress\n",
    "#         with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "#             json.dump(all_annotations, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "#     return all_annotations\n",
    "\n",
    "# # --- Train annotations ---\n",
    "# # train_annotations = process_and_save(train_df, \"content\", \"absa_train.json\", chunk_size=16000)\n",
    "# # train_df[\"annotations\"] = train_annotations\n",
    "\n",
    "# # # --- Test annotations ---\n",
    "# # test_annotations = process_and_save(test_df, \"content\", \"absa_test.json\", chunk_size=16000)\n",
    "# # test_df[\"annotations\"] = test_annotations\n",
    "# # ions ---\n",
    "\n",
    "# # --- Train annotations ---\n",
    "# train_texts = [str(i) for i in train_df[\"content\"].values]\n",
    "# train_annotations = extract_aspects_batch(train_texts)\n",
    "# with open(\"dataset/absa_train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(train_annotations, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# # --- Test annotations ---\n",
    "# test_texts = [str(i) for i in test_df[\"content\"].values]\n",
    "# test_annotations = extract_aspects_batch(test_texts)\n",
    "# with open(\"dataset/absa_test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(test_annotations, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# print(\"‚úÖ Done! Train and test ABSA annotations saved to 'dataset/' folder.\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Convert to dataframe ---\n",
    "# records = []\n",
    "# for ann in annotations:\n",
    "#     text = ann[\"text\"]\n",
    "#     for asp in ann[\"aspects\"]:\n",
    "#         records.append({\n",
    "#             \"text\": text,\n",
    "#             \"aspect\": asp[\"term\"],\n",
    "#             \"from\": asp[\"from\"],\n",
    "#             \"to\": asp[\"to\"],\n",
    "#             \"sentiment\": asp[\"sentiment\"],\n",
    "#             \"score\": asp[\"score\"]\n",
    "#         })\n",
    "\n",
    "# absa_df = pd.DataFrame(records)\n",
    "# absa_df.to_csv(\"absa_model_output.csv\", index=False)\n",
    "print(\"‚úÖ Done! Batch ABSA extraction + sentiment prediction completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fddbbf1-63f1-4fa5-a128-bea1eb02ef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, json, math\n",
    "\n",
    "# def process_and_save(df, col_name, output_json, chunk_size=16000, resume_from=6):\n",
    "#     all_annotations = []\n",
    "#     total = len(df)\n",
    "#     num_chunks = math.ceil(total / chunk_size)\n",
    "\n",
    "#     # --- Resume from file if it exists ---\n",
    "#     if os.path.exists(output_json):\n",
    "#         with open(output_json, \"r\", encoding=\"utf-8\") as f:\n",
    "#             all_annotations = json.load(f)\n",
    "#         if len(all_annotations) // chunk_size > resume_from:\n",
    "#             resume_from = len(all_annotations) // chunk_size\n",
    "#         print(f\"üîÑ Resuming from chunk {resume_from + 1}/{num_chunks}\")\n",
    "#     else:\n",
    "#         print(f\"‚ñ∂Ô∏è Starting from chunk {resume_from + 1}/{num_chunks}\")\n",
    "\n",
    "#     # --- Main loop ---\n",
    "#     for i in range(resume_from, num_chunks):\n",
    "#         start = i * chunk_size\n",
    "#         end = min(start + chunk_size, total)\n",
    "#         print(f\"Processing chunk {i+1}/{num_chunks} ({start}-{end})\")\n",
    "\n",
    "#         # Get text batch\n",
    "#         token_val = [str(t) for t in df[col_name].iloc[start:end].values]\n",
    "\n",
    "#         # Extract aspects\n",
    "#         chunk_annotations = extract_aspects_batch(token_val)\n",
    "\n",
    "#         # Ensure alignment\n",
    "#         if len(chunk_annotations) != len(token_val):\n",
    "#             print(f\"‚ö†Ô∏è Warning: mismatch in chunk {i+1}, padding missing rows\")\n",
    "#             while len(chunk_annotations) < len(token_val):\n",
    "#                 chunk_annotations.append([])\n",
    "\n",
    "#         # Extend and save\n",
    "#         all_annotations.extend(chunk_annotations)\n",
    "#         with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "#             json.dump(all_annotations, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "#     print(\"‚úÖ Processing complete.\")\n",
    "#     return all_annotations\n",
    "\n",
    "\n",
    "# # --- Train annotations ---\n",
    "# train_annotations = process_and_save(train_df, \"content\", \"absa_train.json\", chunk_size=16000)\n",
    "# train_df[\"annotations\"] = train_annotations\n",
    "\n",
    "# # --- Test annotations ---\n",
    "# test_annotations = process_and_save(test_df, \"content\", \"absa_test.json\", chunk_size=16000)\n",
    "# test_df[\"annotations\"] = test_a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42707bc9-3e66-4af9-a957-64855abe2e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # --- Sentiment model ---\n",
    "# sent_model_name = \"yangheng/deberta-v3-base-absa-v1.1\"  # base version to fit GPU\n",
    "# sent_tokenizer = AutoTokenizer.from_pretrained(sent_model_name)\n",
    "# sent_model = AutoModelForSequenceClassification.from_pretrained(sent_model_name).to(device)\n",
    "# sent_model.eval()\n",
    "\n",
    "# # --- Example batch ---\n",
    "# batch_texts = [\n",
    "#     \"The food was great but the service was slow.\",\n",
    "#     \"The battery life of this phone is terrible.\"\n",
    "# ]\n",
    "\n",
    "# # --- Use your existing pipeline for aspect extraction ---\n",
    "# batch_aspects = aspect_pipeline(batch_texts)  # list of list of dicts\n",
    "\n",
    "# # --- Prepare batch for sentiment ---\n",
    "# sent_texts = []\n",
    "# sent_aspects = []\n",
    "# aspect_map = []  # to map back to original text/aspect\n",
    "\n",
    "# for i, (text, aspects) in enumerate(zip(batch_texts, batch_aspects)):\n",
    "#     for aspect in aspects:\n",
    "#         term = aspect[\"word\"]\n",
    "#         sent_texts.append(text)\n",
    "#         sent_aspects.append(term)\n",
    "#         aspect_map.append(i)  # keep track of which text this aspect belongs to\n",
    "\n",
    "# # --- Tokenize for sentiment in batch ---\n",
    "# sent_inputs = sent_tokenizer(sent_texts, text_pair=sent_aspects, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# # --- Forward pass sentiment model ---\n",
    "# with torch.no_grad():\n",
    "#     outputs = sent_model(**sent_inputs)\n",
    "#     probs = torch.softmax(outputs.logits, dim=-1)\n",
    "#     preds = torch.argmax(probs, dim=-1)\n",
    "\n",
    "# # --- Collect results back into batch_texts structure ---\n",
    "# batch_annotations = []\n",
    "        \n",
    "# for i, aspects in enumerate(batch_aspects):  # aspect_pipeline output\n",
    "#     text = batch_texts[i].split()  # tokenized text\n",
    "#     ann = []\n",
    "#     for j, aspect in enumerate(aspects):\n",
    "#         # Sentiment info comes from your batched model\n",
    "#         ann.append({\n",
    "#             \"term\": sent_aspects[i],  # from your batched sentiment list\n",
    "#             \"sentiment\": sent_model.config.id2label[preds[i].item()],\n",
    "#             \"score\": probs[i][preds[i]].item()\n",
    "#         })\n",
    "#     batch_annotations.append({\n",
    "#         \"token\": text,\n",
    "#         \"aspects\": ann\n",
    "#     })\n",
    "\n",
    "# print(batch_annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36e191f-93fa-4770-825b-c1e25f77e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# import torch\n",
    "\n",
    "# # Load tokenizer and model\n",
    "# model_name = \"yangheng/deberta-v3-large-absa-v1.1\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "# model.eval()          # set model to evaluation mode (disables dropout, etc.)\n",
    "# model.to(\"cuda\")      # move model to GPU\n",
    "\n",
    "# # Example batch\n",
    "# texts = [\n",
    "#     \"The food was great but the service was slow.\",\n",
    "#     \"The battery life of this phone is terrible.\"\n",
    "# ]\n",
    "# aspects = [\"food\", \"battery\"]\n",
    "\n",
    "# # Tokenize the batch (text + text_pair)\n",
    "# inputs = tokenizer(texts, text_pair=aspects, padding=True, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# # Forward pass\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "#     logits = outputs.logits\n",
    "\n",
    "# # Convert logits to probabilities / predicted labels\n",
    "# probs = torch.softmax(logits, dim=-1)\n",
    "# preds = torch.argmax(probs, dim=-1)\n",
    "\n",
    "# print(preds, probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01ae17d-bb2a-4e50-b9ac-81aef0054dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c998ad-1d01-44ff-90d3-171f3d618ae6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# # Load dataset\n",
    "# dataset = load_dataset(\"lhoestq/conll2003\")\n",
    "\n",
    "# def tokenize_and_align_labels(batch, tokenizer):\n",
    "#     print(\"\\n=== NEW BATCH ===\")\n",
    "#     print(\"Original tokens example:\")\n",
    "#     print(batch[\"tokens\"][0])  # show first example\n",
    "#     print(\"Original NER tags example:\")\n",
    "#     print(batch[\"ner_tags\"][0])\n",
    "\n",
    "#     # Tokenize the batch\n",
    "#     tokenized_inputs = tokenizer(batch[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "#     print(\"\\nTokenized input keys:\", list(tokenized_inputs.keys()))\n",
    "#     print(\"Example input_ids (first 20):\", tokenized_inputs[\"input_ids\"][0][:20])\n",
    "#     print(\"Example tokens:\", tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"][0][:20]))\n",
    "\n",
    "#     labels = []\n",
    "#     for i, label in enumerate(batch[\"ner_tags\"]):\n",
    "#         print(f'label : {label}')\n",
    "#         word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "#         print(f\"\\nExample {i} word_ids:\", word_ids[:20])\n",
    "\n",
    "#         label_ids = []\n",
    "#         previous_word_id = None\n",
    "#         for word_id in word_ids:\n",
    "#             if word_id is None:\n",
    "#                 # label_ids.append(-100)\n",
    "#                 label_ids.append(0)\n",
    "#             elif word_id != previous_word_id:\n",
    "#                 label_ids.append(label[word_id])\n",
    "#             else:\n",
    "#                 # same word split into multiple tokens, keep same label or -100\n",
    "#                 label_ids.append(label[word_id])\n",
    "#             previous_word_id = word_id\n",
    "\n",
    "#         labels.append(label_ids)\n",
    "#         print(f\"Example {i} label_ids:\", label_ids[:20])\n",
    "\n",
    "#     tokenized_inputs[\"labels\"] = labels\n",
    "#     return tokenized_inputs\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "# print()\n",
    "# # Map the function to a small subset for clarity\n",
    "# tokenized_dataset = dataset.map(\n",
    "#     lambda x: tokenize_and_align_labels(x, tokenizer),\n",
    "#     batched=True\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca0a983f-f274-4aaf-83db-4183a5159528",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\capstone\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "D:\\anaconda3\\envs\\capstone\\Lib\\site-packages\\transformers\\utils\\hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# dataset = load_dataset(\n",
    "#     \"json\",\n",
    "#     data_files={\n",
    "#         \"train\": r\"D:\\Code\\Capstone\\absa_model_output.json\",\n",
    "#     }\n",
    "# )\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "        \"train\": r\"D:\\Code\\Capstone\\dataset\\absa_train.json\",\n",
    "        \"test\":  r\"D:\\Code\\Capstone\\dataset\\absa_test.json\"\n",
    "    }\n",
    ")\n",
    "\n",
    "def find_sequence(lst, seq):\n",
    "    for i in range(len(lst) - len(seq) + 1):\n",
    "        match = True\n",
    "        for j in range(len(seq)):\n",
    "            if seq[j] not in lst[i + j]:\n",
    "                match = False\n",
    "                break\n",
    "        if match:\n",
    "            return i, i + len(seq)\n",
    "    return -1\n",
    "    \n",
    "def tokenize_and_align_labels(batch, tokenizer):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üÜï NEW BATCH\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Show first example info\n",
    "    print(f\"üëâ Example tokens: {batch['token'][0]}\")\n",
    "    print(f\"üëâ Example aspects: {batch['aspects'][0]}\")\n",
    "\n",
    "    # Tokenize the batch\n",
    "    tokenized_inputs = tokenizer(\n",
    "        batch[\"token\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Tokenizer Output ---\")\n",
    "    print(\"Keys:\", list(tokenized_inputs.keys()))\n",
    "    print(\"Example input_ids (first 20):\", tokenized_inputs[\"input_ids\"][0])\n",
    "    print(\"Example tokens (first 20):\", tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"][0]))\n",
    "\n",
    "    labels = []\n",
    "    for i, tokens in enumerate(batch['token']):\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(f\"üîπ Sample {i}\")\n",
    "        print(\"-\"*60)\n",
    "\n",
    "        # Create default label 0 (O-tag)\n",
    "        label = [0] * len(tokens)\n",
    "\n",
    "        # Mark aspect terms with proper label IDs\n",
    "        aspects = batch[\"aspects\"][i]\n",
    "        for aspect in aspects:\n",
    "            # start_id, end_id = aspect['from'], aspect['to']\n",
    "            term = aspect[\"term\"].split()\n",
    "            print(f'term : {term}')\n",
    "            # print(f'find_sequence : {find_sequence(tokens, term)}')\n",
    "            ind_term = find_sequence(tokens, term)\n",
    "            print(f'ind term : {ind_term}')\n",
    "            if ind_term == -1:\n",
    "                continue\n",
    "            start_id, end_id = ind_term\n",
    "            polarity = aspect['sentiment']\n",
    "\n",
    "            if polarity == 'Positive':\n",
    "                label[start_id] = 1  # B-POS\n",
    "                label[start_id + 1:end_id] = [2] * (end_id - start_id - 1)  # I-POS\n",
    "            elif polarity == 'Negative':\n",
    "                label[start_id] = 3  # B-NEG\n",
    "                label[start_id + 1:end_id] = [4] * (end_id - start_id - 1)  # I-NEG\n",
    "            elif polarity == 'Neutral':\n",
    "                label[start_id] = 5  # B-NEU\n",
    "                label[start_id + 1:end_id] = [6] * (end_id - start_id - 1)  # I-NEU\n",
    "\n",
    "        print(f\"üß© Tokens: {tokens}\")\n",
    "        print(f\"üè∑Ô∏è  Word-level labels: {label}\")\n",
    "\n",
    "        # Map labels to tokenized sequence\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        print(f\"üî¢ Word IDs (first 20): {word_ids[:20]}\")\n",
    "\n",
    "        label_ids = []\n",
    "        previous_word_id = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "                # Versu crf\n",
    "                # label_ids.append(0)\n",
    "            elif word_id != previous_word_id:\n",
    "                label_ids.append(label[word_id])\n",
    "            else:\n",
    "                label_ids.append(label[word_id])\n",
    "            previous_word_id = word_id\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "        # Print aligned results side by side for clarity\n",
    "        token_strings = tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"][i])\n",
    "        pairs = list(zip(token_strings, label_ids))\n",
    "        print(\"\\nüßæ Token-label alignment:\")\n",
    "        for tok, lab in pairs:\n",
    "            print(f\"{tok:15} -> {lab}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Final label_ids (first 20): {label_ids[:20]}\")\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "# === Initialize tokenizer ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# === Map with debug printing ===\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda x: tokenize_and_align_labels(x, tokenizer),\n",
    "    batched=True,\n",
    "    # num_proc=2  # <- force single-process to see prints\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f38df01b-5462-423c-b740-d838e9d76c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 4000, Test samples: 1000\n"
     ]
    }
   ],
   "source": [
    "# Total sample size\n",
    "total_sample =5000\n",
    "\n",
    "# Decide split ratio (e.g., 80% train, 20% test)\n",
    "train_ratio = 0.8\n",
    "train_sample_size = int(total_sample * train_ratio)  # 8000\n",
    "test_sample_size  = total_sample - train_sample_size  # 2000\n",
    "\n",
    "# Shuffle and select\n",
    "sampled_train = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(train_sample_size))\n",
    "sampled_test  = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(test_sample_size))\n",
    "\n",
    "print(f\"Train samples: {len(sampled_train)}, Test samples: {len(sampled_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47651ba4-b4e5-4270-973c-da047149baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset(\n",
    "#     \"json\",\n",
    "#     data_files={\n",
    "#         \"train\": r\"D:\\Code\\Capstone\\absa_model_output.json\",\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d5c3f2-be86-4666-ad41-9fc101badea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# # === Load dataset ===\n",
    "# # dataset = load_dataset(\n",
    "# #     \"json\",\n",
    "# #     data_files={\n",
    "# #         \"train\": r\"D:\\Code\\Capstone\\dataset\\Stanford\\Stanford\\Laptops\\train.json\",\n",
    "# #         \"test\": r\"D:\\Code\\Capstone\\dataset\\Stanford\\Stanford\\Laptops\\test.json\",\n",
    "# #     }\n",
    "# # )\n",
    "\n",
    "# def tokenize_and_align_labels(batch, tokenizer):\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"üÜï NEW BATCH\")\n",
    "#     print(\"=\"*80)\n",
    "\n",
    "#     # Show first example info\n",
    "#     print(f\"üëâ Example tokens: {batch['token'][0]}\")\n",
    "#     print(f\"üëâ Example aspects: {batch['aspects'][0]}\")\n",
    "\n",
    "#     # Tokenize the batch\n",
    "#     tokenized_inputs = tokenizer(\n",
    "#         batch[\"token\"],\n",
    "#         truncation=True,\n",
    "#         is_split_into_words=True\n",
    "#     )\n",
    "\n",
    "#     print(\"\\n--- Tokenizer Output ---\")\n",
    "#     print(\"Keys:\", list(tokenized_inputs.keys()))\n",
    "#     print(\"Example input_ids (first 20):\", tokenized_inputs[\"input_ids\"][0][:20])\n",
    "#     print(\"Example tokens (first 20):\", tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"][0][:20]))\n",
    "\n",
    "#     labels = []\n",
    "#     for i, tokens in enumerate(batch['token']):\n",
    "#         print(\"\\n\" + \"-\"*60)\n",
    "#         print(f\"üîπ Sample {i}\")\n",
    "#         print(\"-\"*60)\n",
    "\n",
    "#         # Create default label 0 (O-tag)\n",
    "#         label = [0] * len(tokens)\n",
    "\n",
    "#         # Mark aspect terms with proper label IDs\n",
    "#         aspects = batch[\"aspects\"][i]\n",
    "#         for aspect in aspects:\n",
    "#             start_id, end_id = aspect['from'], aspect['to']\n",
    "#             polarity = aspect['polarity']\n",
    "\n",
    "#             if polarity == 'positive':\n",
    "#                 label[start_id] = 1  # B-POS\n",
    "#                 label[start_id + 1:end_id] = [2] * (end_id - start_id - 1)  # I-POS\n",
    "#             elif polarity == 'negative':\n",
    "#                 label[start_id] = 3  # B-NEG\n",
    "#                 label[start_id + 1:end_id] = [4] * (end_id - start_id - 1)  # I-NEG\n",
    "#             elif polarity == 'neutral':\n",
    "#                 label[start_id] = 5  # B-NEU\n",
    "#                 label[start_id + 1:end_id] = [6] * (end_id - start_id - 1)  # I-NEU\n",
    "\n",
    "#         print(f\"üß© Tokens: {tokens}\")\n",
    "#         print(f\"üè∑Ô∏è  Word-level labels: {label}\")\n",
    "\n",
    "#         # Map labels to tokenized sequence\n",
    "#         word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "#         print(f\"üî¢ Word IDs (first 20): {word_ids[:20]}\")\n",
    "\n",
    "#         label_ids = []\n",
    "#         previous_word_id = None\n",
    "#         for word_id in word_ids:\n",
    "#             if word_id is None:\n",
    "#                 label_ids.append(-100)\n",
    "#                 # Versu crf\n",
    "#                 # label_ids.append(0)\n",
    "#             elif word_id != previous_word_id:\n",
    "#                 label_ids.append(label[word_id])\n",
    "#             else:\n",
    "#                 label_ids.append(label[word_id])\n",
    "#             previous_word_id = word_id\n",
    "\n",
    "#         labels.append(label_ids)\n",
    "\n",
    "#         # Print aligned results side by side for clarity\n",
    "#         token_strings = tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"][i])\n",
    "#         pairs = list(zip(token_strings, label_ids))\n",
    "#         print(\"\\nüßæ Token-label alignment:\")\n",
    "#         for tok, lab in pairs:\n",
    "#             print(f\"{tok:15} -> {lab}\")\n",
    "        \n",
    "#         print(f\"\\n‚úÖ Final label_ids (first 20): {label_ids[:20]}\")\n",
    "\n",
    "#     tokenized_inputs[\"labels\"] = labels\n",
    "#     return tokenized_inputs\n",
    "\n",
    "\n",
    "# # === Initialize tokenizer ===\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# # === Map with debug printing ===\n",
    "# tokenized_dataset = dataset['train'].map(\n",
    "#     lambda x: tokenize_and_align_labels(x, tokenizer),\n",
    "#     batched=True\n",
    "#     # num_proc=2  # <- force single-process to see prints\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ebf4aa2-624f-4ecd-917d-d20fd0ebbd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3843f4f-9195-44ae-a83e-06c50d5aa45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35ad8578-149e-4a78-a6a8-9d955349875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# labels = [label_list[i] for i in example[f\"ner_tags\"]]\n",
    "    \n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # true_predictions = [\n",
    "    #     [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    #     for prediction, label in zip(predictions, labels)\n",
    "    # ]\n",
    "    # true_labels = [\n",
    "    #     [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    #     for prediction, label in zip(predictions, labels)\n",
    "    # ]\n",
    "    true_labels = [\n",
    "    [label_list[min(l, len(label_list)-1)] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    true_predictions = [\n",
    "    [label_list[min(p, len(label_list)-1)] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels, zero_division=0)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d43e1764-5300-42f0-ad85-4b18e4987998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def compute_metrics(p):\n",
    "#     predictions, labels = p\n",
    "#     predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "#     # Align predictions with labels (ignore -100)\n",
    "#     true_predictions = [\n",
    "#         [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "#         for prediction, label in zip(predictions, labels)\n",
    "#     ]\n",
    "#     true_labels = [\n",
    "#         [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "#         for prediction, label in zip(predictions, labels)\n",
    "#     ]\n",
    "    \n",
    "#     # Compute seqeval metrics\n",
    "#     results = seqeval.compute(predictions=true_predictions, references=true_labels, zero_division=0)\n",
    "\n",
    "#     # Add per-class accuracy\n",
    "#     per_class_acc = {}\n",
    "#     for label in label_list:\n",
    "#         # Flatten predictions and labels\n",
    "#         flat_preds = [p for seq in true_predictions for p in seq]\n",
    "#         flat_labels = [l for seq in true_labels for l in seq]\n",
    "#         # Only consider current label\n",
    "#         indices = [i for i, l in enumerate(flat_labels) if l == label]\n",
    "#         if indices:\n",
    "#             correct = sum(flat_preds[i] == flat_labels[i] for i in indices)\n",
    "#             per_class_acc[label] = correct / len(indices)\n",
    "#         else:\n",
    "#             per_class_acc[label] = None  # No samples for this label\n",
    "\n",
    "#     return {\n",
    "#         \"precision\": results[\"overall_precision\"],\n",
    "#         \"recall\": results[\"overall_recall\"],\n",
    "#         \"f1\": results[\"overall_f1\"],\n",
    "#         \"accuracy\": results[\"overall_accuracy\"],\n",
    "#         \"per_class_accuracy\": per_class_acc\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b7efb8d-97ca-4832-9c7e-d5f1da623454",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-positive\",\n",
    "    2: \"I-positive\",\n",
    "    3: \"B-negative\",\n",
    "    4: \"I-negative\",\n",
    "    5: \"B-neutral\",\n",
    "    6: \"I-neutral\",\n",
    "}\n",
    "\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-positive\": 1,\n",
    "    \"I-positive\": 2,\n",
    "    \"B-negative\": 3,\n",
    "    \"I-negative\": 4,\n",
    "    \"B-neutral\": 5,\n",
    "    \"I-neutral\": 6,\n",
    "}\n",
    "\n",
    "label_list = [\n",
    "    \"O\",\n",
    "    \"B-positive\",\n",
    "    \"I-positive\",\n",
    "    \"B-negative\",\n",
    "    \"I-negative\",\n",
    "    \"B-neutral\",\n",
    "    \"I-neutral\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53b18d45-bd0b-487a-899b-c47ad685f61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Untuk dataset conll2003\n",
    "# id2label = {\n",
    "#     0: \"O\",\n",
    "#     1: \"B-LOC\",\n",
    "#     2: \"I-LOC\",\n",
    "#     3: \"B-MISC\",\n",
    "#     4: \"I-MISC\",\n",
    "#     5: \"B-ORG\",\n",
    "#     6: \"I-ORG\",\n",
    "#     7: \"B-PER\",\n",
    "#     8: \"I-PER\"\n",
    "# }\n",
    "\n",
    "# label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e705c8b-2d0b-4f3b-b521-71b3394dfc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution:\n",
      "O               3102703 (83.88%)\n",
      "B-positive      256527 (6.94%)\n",
      "B-neutral       73499 (1.99%)\n",
      "I-neutral       29956 (0.81%)\n",
      "I-positive      132288 (3.58%)\n",
      "B-negative      72508 (1.96%)\n",
      "I-negative      31328 (0.85%)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_label_ratio(dataset, id2label):\n",
    "    # Flatten all label lists (each sample has a list of token labels)\n",
    "    all_labels = [label for example in dataset[\"labels\"] for label in example if label != -100]\n",
    "    counts = Counter(all_labels)\n",
    "    total = sum(counts.values())\n",
    "\n",
    "    print(\"Label distribution:\")\n",
    "    for label_id, count in counts.items():\n",
    "        label_name = id2label.get(label_id, str(label_id))\n",
    "        print(f\"{label_name:<15} {count:>5} ({count / total * 100:.2f}%)\")\n",
    "\n",
    "# Example usage:\n",
    "get_label_ratio(tokenized_dataset['train'], id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d29d01a8-0b01-40b8-8d8d-96ec3882e10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\n",
    "#     \"distilbert/distilbert-base-uncased\", num_labels=7, id2label=id2label, label2id=label2id\n",
    "# )\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=7, id2label=id2label, label2id=label2id\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f46463e4-df29-4709-bb11-bd57c0c8c75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import warnings\n",
    "# import logging\n",
    "# print('hae')\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"seqeval\")\n",
    "# warnings.filterwarnings('always')\n",
    "# # Setup logger\n",
    "# logging.basicConfig(\n",
    "#     format='%(message)s',  # simple output (no timestamp)\n",
    "#     level=logging.INFO\n",
    "# )\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# # Define ranges\n",
    "# lr_range = (1e-5, 5e-5)   # learning rate range\n",
    "# wd_range = (0, 0.3)       # weight decay range\n",
    "# num_runs = 100             # number of random combinations\n",
    "\n",
    "# # Pre-generate random LRs and WDs\n",
    "# log_lr_min, log_lr_max = np.log10(lr_range[0]), np.log10(lr_range[1])\n",
    "\n",
    "# lr_list = []\n",
    "# wd_list = []\n",
    "\n",
    "# for _ in range(num_runs):\n",
    "#     # Random LR in log scale\n",
    "#     log_lr = np.random.uniform(log_lr_min, log_lr_max)\n",
    "#     lr = 10 ** log_lr\n",
    "#     lr_list.append(lr)\n",
    "\n",
    "#     # Random weight decay in linear scale\n",
    "#     wd = np.random.uniform(wd_range[0], wd_range[1])\n",
    "#     wd_list.append(wd)\n",
    "    \n",
    "# for i in range(num_runs):\n",
    "#     print('hae')\n",
    "#     # Generate random values in log scale for lr (common practice)\n",
    "#     lr = lr_list[i]\n",
    "#     wd = wd_list[i]\n",
    "#     # log_lr = np.random.uniform(-6, 1)\n",
    "#     # log_reg = np.random.uniform(-5, 5)\n",
    "    \n",
    "#     # Convert to normal scale\n",
    "#     # lr = 10 ** log_lr\n",
    "#     # reg = 10 ** log_reg\n",
    "\n",
    "#     # ‚úÖ log info in one line like your print version\n",
    "#     # logger.info(f\"Run {i+1} || lr: {lr:.3e} || reg: {wd:.3e}\")\n",
    "#     print(f\"Run {i+1}||\", end='')\n",
    "#     print(f\"lr: {lr:.3e}||\", end='')\n",
    "#     print(f\"reg: {wd:.3e}\", end='')\n",
    "\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=\"my_awesome_wnut_model\",\n",
    "#         learning_rate=lr,\n",
    "#         per_device_train_batch_size=16,\n",
    "#         per_device_eval_batch_size=16,\n",
    "#         num_train_epochs=2,\n",
    "#         weight_decay=wd,\n",
    "#         eval_strategy=\"epoch\",\n",
    "#         save_strategy=\"epoch\",\n",
    "#         load_best_model_at_end=False,\n",
    "#         push_to_hub=False,\n",
    "#     )\n",
    "    \n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=tokenized_dataset[\"train\"],\n",
    "#         eval_dataset=tokenized_dataset[\"test\"],\n",
    "#         processing_class=tokenizer,\n",
    "#         data_collator=data_collator,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#     )\n",
    "    \n",
    "#     trainer.train()\n",
    "#     print('hae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75668634-408c-4b67-9738-68348ce9bd9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import warnings\n",
    "# import logging\n",
    "# print('hae')\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"seqeval\")\n",
    "# warnings.filterwarnings('always')\n",
    "# # Setup logger\n",
    "# logging.basicConfig(\n",
    "#     format='%(message)s',  # simple output (no timestamp)\n",
    "#     level=logging.INFO\n",
    "# )\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# # Define ranges\n",
    "# lr_range = (1e-5, 4e-5)   # learning rate range\n",
    "# wd_range = (0.006, 0.1)       # weight decay range\n",
    "# num_runs = 100             # number of random combinations\n",
    "\n",
    "# # Pre-generate random LRs and WDs\n",
    "# log_lr_min, log_lr_max = np.log10(lr_range[0]), np.log10(lr_range[1])\n",
    "\n",
    "# lr_list = []\n",
    "# wd_list = []\n",
    "\n",
    "# for _ in range(num_runs):\n",
    "#     # Random LR in log scale\n",
    "#     log_lr = np.random.uniform(log_lr_min, log_lr_max)\n",
    "#     lr = 10 ** log_lr\n",
    "#     lr_list.append(lr)\n",
    "\n",
    "#     # Random weight decay in linear scale\n",
    "#     wd = np.random.uniform(wd_range[0], wd_range[1])\n",
    "#     wd_list.append(wd)\n",
    "    \n",
    "# for i in range(num_runs):\n",
    "#     print('hae')\n",
    "#     # Generate random values in log scale for lr (common practice)\n",
    "#     lr = lr_list[i]\n",
    "#     wd = wd_list[i]\n",
    "#     # log_lr = np.random.uniform(-6, 1)\n",
    "#     # log_reg = np.random.uniform(-5, 5)\n",
    "    \n",
    "#     # Convert to normal scale\n",
    "#     # lr = 10 ** log_lr\n",
    "#     # reg = 10 ** log_reg\n",
    "\n",
    "#     # ‚úÖ log info in one line like your print version\n",
    "#     # logger.info(f\"Run {i+1} || lr: {lr:.3e} || reg: {wd:.3e}\")\n",
    "#     print(f\"Run {i+1}||\", end='')\n",
    "#     print(f\"lr: {lr:.3e}||\", end='')\n",
    "#     print(f\"reg: {wd:.3e}\", end='')\n",
    "\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=\"my_awesome_wnut_model\",\n",
    "#         learning_rate=lr,\n",
    "#         per_device_train_batch_size=16,\n",
    "#         per_device_eval_batch_size=16,\n",
    "#         num_train_epochs=10,\n",
    "#         weight_decay=wd,\n",
    "#         eval_strategy=\"epoch\",\n",
    "#         save_strategy=\"epoch\",\n",
    "#         load_best_model_at_end=False,\n",
    "#         push_to_hub=False,\n",
    "#     )\n",
    "    \n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=tokenized_dataset[\"train\"],\n",
    "#         eval_dataset=tokenized_dataset[\"test\"],\n",
    "#         processing_class=tokenizer,\n",
    "#         data_collator=data_collator,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#     )\n",
    "    \n",
    "#     trainer.train()\n",
    "#     print('hae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5804ec5e-de1a-43eb-932a-55c5f4d432b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from collections import Counter\n",
    "\n",
    "# # your label distribution (ignore -100)\n",
    "# counts = {\n",
    "#     \"O\": 3102703,\n",
    "#     \"B-positive\": 256527,\n",
    "#     \"B-neutral\": 73499,\n",
    "#     \"I-neutral\": 29956,\n",
    "#     \"I-positive\": 132288,\n",
    "#     \"B-negative\": 72508,\n",
    "#     \"I-negative\": 31328,\n",
    "# }\n",
    "\n",
    "# # inverse frequency weights\n",
    "# total = sum(counts.values())\n",
    "# weights = {label: total / count for label, count in counts.items()}\n",
    "\n",
    "# # normalize so weights roughly sum to num_labels (optional)\n",
    "# norm_factor = sum(weights.values()) / len(weights)\n",
    "# weights = {label: w / norm_factor for label, w in weights.items()}\n",
    "\n",
    "# # convert to tensor (make sure order matches label2id)\n",
    "# weights_tensor = torch.tensor([weights[label] for label in label2id.keys()], dtype=torch.float)\n",
    "# print(weights_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93d5fe6a-3076-4c90-a1ed-532e95e71bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Trainer\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # Example weights: class 0 is 1, others 1‚Äì6 are weighted more\n",
    "        class_weights = torch.tensor([0.0216, 0.2612, 0.5065, 0.9240, 2.1386, 0.9116, 2.2366]).to(logits.device)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        \n",
    "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "# 2.273e-05,\n",
    "#     per_device_train_batch_size=32,\n",
    "#     per_device_eval_batch_size=32,\n",
    "#     num_train_epochs=1000,\n",
    "#     weight_decay=1.037e-02,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f2a5674-6edc-4cb4-9702-5f59afe7e739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import warnings\n",
    "# import logging\n",
    "# from transformers import EarlyStoppingCallback\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"seqeval\")\n",
    "# warnings.filterwarnings('always')\n",
    "\n",
    "# def training_weighted(model, num_runs, epoch):\n",
    "#     print('Start')\n",
    "#     # Setup logger\n",
    "#     logging.basicConfig(\n",
    "#         format='%(message)s',  # simple output (no timestamp)\n",
    "#         level=logging.INFO\n",
    "#     )\n",
    "#     logger = logging.getLogger(__name__)\n",
    "    \n",
    "#     # Define ranges\n",
    "#     lr_range = (1e-5, 3e-5)     # learning rate\n",
    "#     wd_range = (0.0, 0.05)      # weight decay\n",
    "#     num_runs = 100             # number of random combinations\n",
    "    \n",
    "#     # Pre-generate random LRs and WDs\n",
    "#     log_lr_min, log_lr_max = np.log10(lr_range[0]), np.log10(lr_range[1])\n",
    "    \n",
    "#     lr_list = []\n",
    "#     wd_list = []\n",
    "#     for _ in range(num_runs):\n",
    "#            # Random LR in log scale\n",
    "#         log_lr = np.random.uniform(log_lr_min, log_lr_max)\n",
    "#         lr = 10 ** log_lr\n",
    "#         lr_list.append(lr)\n",
    "    \n",
    "#         # Random weight decay in linear scale\n",
    "#         wd = np.random.uniform(wd_range[0], wd_range[1])\n",
    "#         wd_list.append(wd)\n",
    "\n",
    "#     for i in range(num_runs):\n",
    "#         lr = lr_list[i]\n",
    "#         wd = wd_list[i]\n",
    "#         lr = 1.509e-05\n",
    "#         wd = 4.754e-02\n",
    "#         # ‚úÖ log info in one line like your print version\n",
    "#         # logger.info(f\"Run {i+1} || lr: {lr:.3e} || reg: {wd:.3e}\")\n",
    "#         print(f\"Run {i+1}||\", end='')\n",
    "#         print(f\"lr: {lr:.3e}||\", end='')\n",
    "#         print(f\"reg: {wd:.3e}\", end='')\n",
    "    \n",
    "        \n",
    "#         training_args = TrainingArguments(\n",
    "#             output_dir=\"my_awesome_wnut_model\",\n",
    "#             learning_rate=lr,\n",
    "#             per_device_train_batch_size=16,\n",
    "#             per_device_eval_batch_size=16,\n",
    "#             num_train_epochs=epoch,\n",
    "#             weight_decay=wd,\n",
    "#             eval_strategy=\"epoch\",\n",
    "#             save_strategy=\"epoch\",\n",
    "#             metric_for_best_model=\"f1\",\n",
    "#             load_best_model_at_end=True,\n",
    "#             push_to_hub=True,\n",
    "#         )\n",
    "\n",
    "#         early_stopping_callback = EarlyStoppingCallback(\n",
    "#             early_stopping_patience=3,\n",
    "#             early_stopping_threshold=0.01\n",
    "#         )\n",
    "    \n",
    "#         trainer = WeightedTrainer(\n",
    "#             model=model,\n",
    "#             args=training_args,\n",
    "#             train_dataset=tokenized_dataset['train'],\n",
    "#             eval_dataset=tokenized_dataset['test'],\n",
    "#             processing_class=tokenizer,\n",
    "#             data_collator=data_collator,\n",
    "#             compute_metrics=compute_metrics,\n",
    "#             callbacks=[early_stopping_callback]\n",
    "#         )\n",
    "        \n",
    "#         trainer.train()\n",
    "\n",
    "# training_weighted(model=model, num_runs=1, epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5438c5b-94bc-441b-9f31-86f7314cd0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "def model_init():\n",
    "    return AutoModelForTokenClassification.from_pretrained(model_name, num_labels=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53f61949-7086-4f8d-b093-d55c30c9b8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 12:46:27,885] Using an existing study with name 'transformers_optuna_studys1' instead of creating a new one.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.storages import RDBStorage\n",
    "\n",
    "# Define persistent storage\n",
    "storage = RDBStorage(\"sqlite:///optuna_trials.db\")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=\"transformers_optuna_studys1\",\n",
    "    direction=\"maximize\",\n",
    "    storage=storage,\n",
    "    load_if_exists=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8009b96d-efd6-4657-9fe7-ceeb053d7f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\capstone\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "D:\\anaconda3\\envs\\capstone\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mramapanjinararendra\u001b[0m (\u001b[33mramapanjinararendra-uty\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\Code\\Capstone\\wandb\\run-20251022_124635-55algd3d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ramapanjinararendra-uty/hf-optuna/runs/55algd3d' target=\"_blank\">transformers_optuna_study</a></strong> to <a href='https://wandb.ai/ramapanjinararendra-uty/hf-optuna' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ramapanjinararendra-uty/hf-optuna' target=\"_blank\">https://wandb.ai/ramapanjinararendra-uty/hf-optuna</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ramapanjinararendra-uty/hf-optuna/runs/55algd3d' target=\"_blank\">https://wandb.ai/ramapanjinararendra-uty/hf-optuna/runs/55algd3d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "import gc\n",
    "\n",
    "# metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions = eval_pred.predictions.argmax(axis=-1)\n",
    "#     labels = eval_pred.label_ids\n",
    "#     return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "def compute_objective(metrics):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return metrics.get(\"f1\", 0.0)  # fallback to 0 if 'f1' is missing\n",
    "\n",
    "# def objective(trial):\n",
    "#     # free memory at the start of each trial\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     # define hyperparameters, create trainer, etc.\n",
    "#     trainer = Trainer(\n",
    "#         model_init=model_init,\n",
    "#         args=training_args,\n",
    "#         train_dataset=sampled_train,\n",
    "#         eval_dataset=sampled_test,\n",
    "#         tokenizer=tokenizer,\n",
    "#         data_collator=data_collator,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#     )\n",
    "\n",
    "#     trainer.train()\n",
    "#     metrics = trainer.evaluate()\n",
    "#     return metrics[\"f1\"]\n",
    "\n",
    "wandb.init(project=\"hf-optuna\", name=\"transformers_optuna_study\")\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#         eval_strategy=\"epoch\",\n",
    "#         save_strategy=\"epoch\",\n",
    "#         load_best_model_at_end=True,\n",
    "#         logging_strategy=\"epoch\",\n",
    "#         num_train_epochs=3,\n",
    "#         report_to=\"wandb\",  # Logs to W&B\n",
    "#         logging_dir=\"./logs\",\n",
    "#         run_name=\"transformers_optuna_study\",\n",
    "# )\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_wnut_model\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"wandb\",  # Logs to W&B\n",
    "    logging_dir=\"./logs\",\n",
    "    run_name=\"transformers_optuna_study\",\n",
    "    metric_for_best_model=\"f1\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.01\n",
    ")\n",
    "# trainer = Trainer(\n",
    "#     model_init=model_init,\n",
    "#     args=training_args,\n",
    "#     train_dataset=sampled_train,\n",
    "#     eval_dataset=sampled_test,\n",
    "#     processing_class=tokenizer,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=sampled_train,\n",
    "    eval_dataset=sampled_test,\n",
    "    data_collator=data_collator, \n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "529e829e-ab0f-4c73-9609-a672b0e3c6d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 12:46:48,069] Using an existing study with name 'transformers_optuna_studys1' instead of creating a new one.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">transformers_optuna_study</strong> at: <a href='https://wandb.ai/ramapanjinararendra-uty/hf-optuna/runs/55algd3d' target=\"_blank\">https://wandb.ai/ramapanjinararendra-uty/hf-optuna/runs/55algd3d</a><br> View project at: <a href='https://wandb.ai/ramapanjinararendra-uty/hf-optuna' target=\"_blank\">https://wandb.ai/ramapanjinararendra-uty/hf-optuna</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251022_124635-55algd3d\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\Code\\Capstone\\wandb\\run-20251022_124652-uc21rg9m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ramapanjinararendra-uty/huggingface/runs/uc21rg9m' target=\"_blank\">unique-elevator-17</a></strong> to <a href='https://wandb.ai/ramapanjinararendra-uty/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ramapanjinararendra-uty/huggingface' target=\"_blank\">https://wandb.ai/ramapanjinararendra-uty/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ramapanjinararendra-uty/huggingface/runs/uc21rg9m' target=\"_blank\">https://wandb.ai/ramapanjinararendra-uty/huggingface/runs/uc21rg9m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 07:15, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.734900</td>\n",
       "      <td>0.619805</td>\n",
       "      <td>0.379032</td>\n",
       "      <td>0.052765</td>\n",
       "      <td>0.092634</td>\n",
       "      <td>0.842621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.629900</td>\n",
       "      <td>0.606348</td>\n",
       "      <td>0.365815</td>\n",
       "      <td>0.064272</td>\n",
       "      <td>0.109334</td>\n",
       "      <td>0.843657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 13:06:03,102] Trial 6 finished with value: 0.0 and parameters: {'learning_rate': 1.7722789866867935e-06, 'weight_decay': 0.15735281405550236}. Best is trial 2 with value: 0.0.\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001DB995E20F0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 04f3d691-eacc-486a-92f5-7557ca41524f)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001DCE43CEB70>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: f2e5504f-7160-4f59-b652-4608660685c0)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001DBDB4CE1B0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 8a206c89-23fb-4c53-884d-ec6436da5a87)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001DBDB4CEC30>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 3580d9a1-9d42-426e-9703-e037bd2f6bb2)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001DBC3C1B7A0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 0d2fb615-c3ed-4e0e-8954-64f2d1f316a6)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001DCE43CF530>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: deee725f-6b46-4506-941f-7a023d23b090)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÅ</td></tr><tr><td>eval/precision</td><td>‚ñà‚ñÅ</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñà‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ‚ñà</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ‚ñà</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñà‚ñà‚ñà</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.84366</td></tr><tr><td>eval/f1</td><td>0.10933</td></tr><tr><td>eval/loss</td><td>0.60635</td></tr><tr><td>eval/precision</td><td>0.36581</td></tr><tr><td>eval/recall</td><td>0.06427</td></tr><tr><td>eval/runtime</td><td>7.8521</td></tr><tr><td>eval/samples_per_second</td><td>127.355</td></tr><tr><td>eval/steps_per_second</td><td>15.919</td></tr><tr><td>total_flos</td><td>403656411307776.0</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>+8</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unique-elevator-17</strong> at: <a href='https://wandb.ai/ramapanjinararendra-uty/huggingface/runs/uc21rg9m' target=\"_blank\">https://wandb.ai/ramapanjinararendra-uty/huggingface/runs/uc21rg9m</a><br> View project at: <a href='https://wandb.ai/ramapanjinararendra-uty/huggingface' target=\"_blank\">https://wandb.ai/ramapanjinararendra-uty/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251022_124652-uc21rg9m\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\Code\\Capstone\\wandb\\run-20251022_130802-kodlj7p1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ramapanjinararendra-uty/huggingface/runs/kodlj7p1' target=\"_blank\">young-cloud-18</a></strong> to <a href='https://wandb.ai/ramapanjinararendra-uty/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ramapanjinararendra-uty/huggingface' target=\"_blank\">https://wandb.ai/ramapanjinararendra-uty/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ramapanjinararendra-uty/huggingface/runs/kodlj7p1' target=\"_blank\">https://wandb.ai/ramapanjinararendra-uty/huggingface/runs/kodlj7p1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 04:31, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.585100</td>\n",
       "      <td>0.505770</td>\n",
       "      <td>0.471585</td>\n",
       "      <td>0.242212</td>\n",
       "      <td>0.320045</td>\n",
       "      <td>0.860253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.482100</td>\n",
       "      <td>0.486397</td>\n",
       "      <td>0.456592</td>\n",
       "      <td>0.278978</td>\n",
       "      <td>0.346341</td>\n",
       "      <td>0.863268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 13:28:53,031] Trial 7 finished with value: 0.0 and parameters: {'learning_rate': 3.12708877756592e-05, 'weight_decay': 0.2610812306412521}. Best is trial 2 with value: 0.0.\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001DBC413E840>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 9d7472e1-f2b6-41bb-8e84-70687c9ef66d)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001DBDB652000>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: b9762166-f9ce-41a2-bbbf-7210adbdf354)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001DBDB651B20>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: b540e593-98a1-49b8-9e3c-a435431069b9)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001DBC413D880>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 67c14fe2-a12a-46e7-92db-44a9412b070a)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÅ</td></tr><tr><td>eval/precision</td><td>‚ñà‚ñÅ</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñÅ‚ñà</td></tr><tr><td>eval/samples_per_second</td><td>‚ñà‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñà‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñà‚ñà‚ñà</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.86327</td></tr><tr><td>eval/f1</td><td>0.34634</td></tr><tr><td>eval/loss</td><td>0.4864</td></tr><tr><td>eval/precision</td><td>0.45659</td></tr><tr><td>eval/recall</td><td>0.27898</td></tr><tr><td>eval/runtime</td><td>7.9406</td></tr><tr><td>eval/samples_per_second</td><td>125.935</td></tr><tr><td>eval/steps_per_second</td><td>15.742</td></tr><tr><td>total_flos</td><td>403656411307776.0</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>+8</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">young-cloud-18</strong> at: <a href='https://wandb.ai/ramapanjinararendra-uty/huggingface/runs/kodlj7p1' target=\"_blank\">https://wandb.ai/ramapanjinararendra-uty/huggingface/runs/kodlj7p1</a><br> View project at: <a href='https://wandb.ai/ramapanjinararendra-uty/huggingface' target=\"_blank\">https://wandb.ai/ramapanjinararendra-uty/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251022_130802-kodlj7p1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\Code\\Capstone\\wandb\\run-20251022_133202-4gzrl2ox</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ramapanjinararendra-uty/huggingface/runs/4gzrl2ox' target=\"_blank\">wild-moon-19</a></strong> to <a href='https://wandb.ai/ramapanjinararendra-uty/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ramapanjinararendra-uty/huggingface' target=\"_blank\">https://wandb.ai/ramapanjinararendra-uty/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ramapanjinararendra-uty/huggingface/runs/4gzrl2ox' target=\"_blank\">https://wandb.ai/ramapanjinararendra-uty/huggingface/runs/4gzrl2ox</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 04:55, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.581700</td>\n",
       "      <td>0.493430</td>\n",
       "      <td>0.473469</td>\n",
       "      <td>0.260455</td>\n",
       "      <td>0.336049</td>\n",
       "      <td>0.861806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.462500</td>\n",
       "      <td>0.476429</td>\n",
       "      <td>0.455504</td>\n",
       "      <td>0.303115</td>\n",
       "      <td>0.364004</td>\n",
       "      <td>0.864913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 16:18:28,340] Trial 8 finished with value: 0.0 and parameters: {'learning_rate': 6.657741827035944e-05, 'weight_decay': 0.17212889998541997}. Best is trial 2 with value: 0.0.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÅ</td></tr><tr><td>eval/precision</td><td>‚ñà‚ñÅ</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñà‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ‚ñà</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ‚ñà</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñà‚ñà‚ñà</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.86491</td></tr><tr><td>eval/f1</td><td>0.364</td></tr><tr><td>eval/loss</td><td>0.47643</td></tr><tr><td>eval/precision</td><td>0.4555</td></tr><tr><td>eval/recall</td><td>0.30312</td></tr><tr><td>eval/runtime</td><td>8.0486</td></tr><tr><td>eval/samples_per_second</td><td>124.246</td></tr><tr><td>eval/steps_per_second</td><td>15.531</td></tr><tr><td>total_flos</td><td>403656411307776.0</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>+8</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wild-moon-19</strong> at: <a href='https://wandb.ai/ramapanjinararendra-uty/huggingface/runs/4gzrl2ox' target=\"_blank\">https://wandb.ai/ramapanjinararendra-uty/huggingface/runs/4gzrl2ox</a><br> View project at: <a href='https://wandb.ai/ramapanjinararendra-uty/huggingface' target=\"_blank\">https://wandb.ai/ramapanjinararendra-uty/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251022_133202-4gzrl2ox\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\Code\\Capstone\\wandb\\run-20251022_161842-s25ihyt7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ramapanjinararendra-uty/huggingface/runs/s25ihyt7' target=\"_blank\">sandy-hill-20</a></strong> to <a href='https://wandb.ai/ramapanjinararendra-uty/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ramapanjinararendra-uty/huggingface' target=\"_blank\">https://wandb.ai/ramapanjinararendra-uty/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ramapanjinararendra-uty/huggingface/runs/s25ihyt7' target=\"_blank\">https://wandb.ai/ramapanjinararendra-uty/huggingface/runs/s25ihyt7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 05:32, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.602689</td>\n",
       "      <td>0.366366</td>\n",
       "      <td>0.068482</td>\n",
       "      <td>0.115394</td>\n",
       "      <td>0.843961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.608100</td>\n",
       "      <td>0.585643</td>\n",
       "      <td>0.346584</td>\n",
       "      <td>0.078305</td>\n",
       "      <td>0.127747</td>\n",
       "      <td>0.844570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 16:37:29,211] Trial 9 finished with value: 0.0 and parameters: {'learning_rate': 3.1066785998883832e-06, 'weight_decay': 0.1755409568247051}. Best is trial 2 with value: 0.0.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÅ</td></tr><tr><td>eval/precision</td><td>‚ñà‚ñÅ</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñÅ‚ñà</td></tr><tr><td>eval/samples_per_second</td><td>‚ñà‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñà‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñà‚ñà‚ñà</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.84457</td></tr><tr><td>eval/f1</td><td>0.12775</td></tr><tr><td>eval/loss</td><td>0.58564</td></tr><tr><td>eval/precision</td><td>0.34658</td></tr><tr><td>eval/recall</td><td>0.0783</td></tr><tr><td>eval/runtime</td><td>14.7985</td></tr><tr><td>eval/samples_per_second</td><td>67.574</td></tr><tr><td>eval/steps_per_second</td><td>8.447</td></tr><tr><td>total_flos</td><td>403656411307776.0</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>+8</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sandy-hill-20</strong> at: <a href='https://wandb.ai/ramapanjinararendra-uty/huggingface/runs/s25ihyt7' target=\"_blank\">https://wandb.ai/ramapanjinararendra-uty/huggingface/runs/s25ihyt7</a><br> View project at: <a href='https://wandb.ai/ramapanjinararendra-uty/huggingface' target=\"_blank\">https://wandb.ai/ramapanjinararendra-uty/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251022_161842-s25ihyt7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\Code\\Capstone\\wandb\\run-20251022_163736-bo66r8zi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ramapanjinararendra-uty/huggingface/runs/bo66r8zi' target=\"_blank\">polar-energy-21</a></strong> to <a href='https://wandb.ai/ramapanjinararendra-uty/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ramapanjinararendra-uty/huggingface' target=\"_blank\">https://wandb.ai/ramapanjinararendra-uty/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ramapanjinararendra-uty/huggingface/runs/bo66r8zi' target=\"_blank\">https://wandb.ai/ramapanjinararendra-uty/huggingface/runs/bo66r8zi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 05:05, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.666800</td>\n",
       "      <td>0.590182</td>\n",
       "      <td>0.356671</td>\n",
       "      <td>0.075779</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.844692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.591900</td>\n",
       "      <td>0.569631</td>\n",
       "      <td>0.374449</td>\n",
       "      <td>0.095425</td>\n",
       "      <td>0.152091</td>\n",
       "      <td>0.845849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 16:56:52,586] Trial 10 finished with value: 0.0 and parameters: {'learning_rate': 4.602135761658058e-06, 'weight_decay': 0.1298642873123391}. Best is trial 2 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BestRun(run_id='2', objective=0.0, hyperparameters={'learning_rate': 1.9283721133035465e-06, 'weight_decay': 0.20220428470198348}, run_summary=None)\n"
     ]
    }
   ],
   "source": [
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        # \"per_device_train_batch_size\": trial.suggest_categorical(\n",
    "        #     \"per_device_train_batch_size\", [8, 16]\n",
    "        # ),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.3),\n",
    "    }\n",
    "\n",
    "\n",
    "best_run = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=5,\n",
    "    compute_objective=compute_objective,\n",
    "    study_name=\"transformers_optuna_studys1\",\n",
    "    storage=\"sqlite:///optuna_trials.db\",\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "533b2170-60db-460f-9d2d-2f35f5cf0de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['python', '-m', 'optuna.dashboard', 'sqlite:...>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Stop previous dashboard if running\n",
    "# On Windows, just restart the kernel if needed\n",
    "\n",
    "# Start dashboard in background\n",
    "subprocess.Popen([\n",
    "    \"python\", \"-m\", \"optuna.dashboard\",\n",
    "    \"sqlite:///optuna_trials.db\",\n",
    "    \"--port\", \"8080\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1a4c6cf-a7e7-4a8f-bd4c-e39b11119376",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptuna\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptuna\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvisualization\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     plot_optimization_history,\n\u001b[32m      4\u001b[39m     plot_intermediate_values,\n\u001b[32m      5\u001b[39m     plot_param_importances\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Load the study from RDB storage\u001b[39;00m\n\u001b[32m     10\u001b[39m storage = optuna.storages.RDBStorage(\u001b[33m\"\u001b[39m\u001b[33msqlite:///optuna_trials.db\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.visualization.matplotlib import (\n",
    "    plot_optimization_history,\n",
    "    plot_intermediate_values,\n",
    "    plot_param_importances\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the study from RDB storage\n",
    "storage = optuna.storages.RDBStorage(\"sqlite:///optuna_trials.db\")\n",
    "\n",
    "study = optuna.load_study(\n",
    "    study_name=\"transformers_optuna_study\",\n",
    "    storage=storage\n",
    ")\n",
    "\n",
    "# Plot optimization history\n",
    "ax1 = plot_optimization_history(study)\n",
    "plt.show()\n",
    "ax1.figure.savefig(\"optimization_history.png\")\n",
    "\n",
    "# Plot intermediate values (if using pruning and intermediate reports)\n",
    "ax2 = plot_intermediate_values(study)\n",
    "plt.show()\n",
    "ax2.figure.savefig(\"intermediate_values.png\")\n",
    "\n",
    "# Plot parameter importances\n",
    "ax3 = plot_param_importances(study)\n",
    "plt.show()\n",
    "ax3.figure.savefig(\"param_importances.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "17ab19e7-a144-493f-a95a-92d69fbb8302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "class DistilBERT_CRF(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(pretrained_model_name)\n",
    "        # self.hidden2tag = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        # self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        print(\"üöÄ Forward started\")\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        # sequence_output = outputs.last_hidden_state  # [batch, seq_len, hidden_size]\n",
    "        # emissions = self.hidden2tag(sequence_output) # [batch, seq_len, num_labels]\n",
    "        # print(\"‚úÖ BERT + Linear done\")\n",
    "\n",
    "        # if labels is not None:\n",
    "        #     # üß† Debug section\n",
    "        #     print(\"‚öôÔ∏è  DEBUG:\")\n",
    "        #     print(f\"emissions shape: {emissions.shape}\")\n",
    "        #     print(f\"labels shape: {labels.shape}\")\n",
    "        #     print(f\"mask shape: {attention_mask.shape}\")\n",
    "        #     print(f\"Min label: {labels.min().item()}, Max label: {labels.max().item()}\")\n",
    "        #     print(f\"Num labels in CRF: {self.crf.num_tags}\")\n",
    "        #     assert labels.shape == attention_mask.shape, \"‚ùå Label and mask must have same shape\"\n",
    "        #     assert emissions.shape[:2] == labels.shape, \"‚ùå Emission batch/seq must match labels\"\n",
    "\n",
    "        #     # Compute CRF loss\n",
    "        #     loss = -self.crf(emissions, labels, mask=attention_mask.bool(), reduction='mean')\n",
    "        #     return loss\n",
    "        # else:\n",
    "        #     # Decode tags\n",
    "        #     prediction = self.crf.decode(emissions, mask=attention_mask.bool())\n",
    "            # return prediction\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6bfede1a-7aa3-4d82-86f8-7db286c48252",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBERT_CRF(\"distilbert/distilbert-base-uncased\", 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "344fb86a-b4e3-4da0-8ef4-bed7c48f3074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset.set_format(\n",
    "    type='torch',\n",
    "    columns=['input_ids', 'attention_mask', 'labels']\n",
    ")\n",
    "print(type(labels), type(input_ids), type(attention_mask))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "86d97133-aa6e-4556-be78-0b6febfc7a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "25f920c8-83a8-46ad-8aa9-414951fde27f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:778\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[1;32m--> 778\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:740\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[1;34m(value, dtype)\u001b[0m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[1;32m--> 740\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 4 at dim 1 (got 3)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# === Run single batch forward ===\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43müîπ Batch shapes:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\data_collator.py:272\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 272\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m    281\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\data_collator.py:67\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[1;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3386\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   3383\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   3384\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[1;32m-> 3386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:242\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[1;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[0;32m    238\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[1;32m--> 242\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:794\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    790\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    791\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    792\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    793\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m--> 794\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    795\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    796\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    797\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    798\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    799\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# === Load model & tokenizer ===\n",
    "# model_name = \"bert-base-uncased\"  # change to your model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=7)\n",
    "\n",
    "# === Example dataset ===\n",
    "texts = [\"I love PyTorch\", \"Transformers are powerful\"]\n",
    "labels = [[1, 2, 0, 0], [1, 0, 0]]  # dummy labels\n",
    "\n",
    "# === Tokenize ===\n",
    "tokenized_dataset = tokenizer(texts, truncation=True, padding=False, return_tensors=None)\n",
    "\n",
    "# === Make PyTorch dataset ===\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "dataset = MyDataset(tokenized_dataset, labels)\n",
    "\n",
    "# === DataLoader with automatic padding ===\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "loader = DataLoader(dataset, batch_size=2, collate_fn=data_collator)\n",
    "\n",
    "# === Move model to device ===\n",
    "device = 'cpu'\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# === Run single batch forward ===\n",
    "for batch in loader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    print(\"\\nüîπ Batch shapes:\", {k: v.shape for k, v in batch.items()})\n",
    "    print(\"Device:\", next(model.parameters()).device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            output = model(**batch)\n",
    "            print(\"‚úÖ Forward successful\")\n",
    "            if hasattr(output, \"loss\"):\n",
    "                print(f\"Loss: {output.loss.item()}\")\n",
    "        except Exception as e:\n",
    "            print(\"‚ùå Forward failed:\", e)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "90c05c2c-a4f9-467d-a6ba-d54b1713c0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cbdc70a3-71ed-4959-bf24-4d7ea4096099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hae\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='440' max='4390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 440/4390 06:35 < 59:24, 1.11 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='108' max='108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [108/108 01:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 70\u001b[0m\n\u001b[0;32m     46\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     47\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_awesome_wnut_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     48\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     push_to_hub\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     58\u001b[0m )\n\u001b[0;32m     60\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     61\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     62\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     68\u001b[0m )\n\u001b[1;32m---> 70\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhae\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2240\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2238\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2245\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2656\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2653\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2656\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\n\u001b[0;32m   2658\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   2661\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[0;32m   2662\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:3095\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[0;32m   3093\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[1;32m-> 3095\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3096\u001b[0m     is_new_best_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determine_best_metric(metrics\u001b[38;5;241m=\u001b[39mmetrics, trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m   3098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m==\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:3044\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[1;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[0;32m   3043\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m-> 3044\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3045\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   3047\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:4173\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4170\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   4172\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 4173\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4174\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   4177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   4178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4181\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4183\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   4184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:4463\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4461\u001b[0m     eval_set_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_losses \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4462\u001b[0m     eval_set_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_inputs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 4463\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meval_set_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4465\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4466\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4467\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[39], line 11\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m      7\u001b[0m predictions, labels \u001b[38;5;241m=\u001b[39m p\n\u001b[0;32m      8\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     10\u001b[0m true_predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 11\u001b[0m     [\u001b[43mlabel_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m (p, l) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prediction, label) \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m]\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prediction, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels)\n\u001b[0;32m     13\u001b[0m ]\n\u001b[0;32m     14\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     15\u001b[0m     [label_list[l] \u001b[38;5;28;01mfor\u001b[39;00m (p, l) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prediction, label) \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m]\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prediction, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels)\n\u001b[0;32m     17\u001b[0m ]\n\u001b[0;32m     19\u001b[0m results \u001b[38;5;241m=\u001b[39m seqeval\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mtrue_predictions, references\u001b[38;5;241m=\u001b[39mtrue_labels, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import logging\n",
    "print('hae')\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"seqeval\")\n",
    "warnings.filterwarnings('always')\n",
    "# Setup logger\n",
    "logging.basicConfig(\n",
    "    format='%(message)s',  # simple output (no timestamp)\n",
    "    level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define ranges\n",
    "# lr_range = (1e-5, 4e-5)   # learning rate range\n",
    "# wd_range = (0.006, 0.1)       # weight decay range\n",
    "# num_runs = 100             # number of random combinations\n",
    "\n",
    "# # Pre-generate random LRs and WDs\n",
    "# log_lr_min, log_lr_max = np.log10(lr_range[0]), np.log10(lr_range[1])\n",
    "\n",
    "# lr_list = []\n",
    "# wd_list = []\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# # Generate random values in log scale for lr (common practice)\n",
    "# lr = lr_list[i]\n",
    "# wd = wd_list[i]\n",
    "# log_lr = np.random.uniform(-6, 1)\n",
    "# log_reg = np.random.uniform(-5, 5)\n",
    "\n",
    "# Convert to normal scale\n",
    "# lr = 10 ** log_lr\n",
    "# reg = 10 ** log_reg\n",
    "\n",
    "# ‚úÖ log info in one line like your print version\n",
    "# logger.info(f\"Run {i+1} || lr: {lr:.3e} || reg: {wd:.3e}\")\n",
    "# print(f\"Run {i+1}||\", end='')\n",
    "# print(f\"lr: {lr:.3e}||\", end='')\n",
    "# print(f\"reg: {wd:.3e}\", end='')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_wnut_model\",\n",
    "    learning_rate=2e-5,\n",
    "    no_cuda=False,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print('hae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f1afa6a9-9863-4628-a854-91a883b336a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(\n",
    "    type='torch',\n",
    "    columns=['input_ids', 'attention_mask', 'labels']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b113b69a-8e14-41d7-a7c7-b50701737b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# ensure your tokenized dataset outputs torch tensors\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# data collator for padding\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "61001756-31cc-4ad7-a55c-9a41b1abc997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cpu\n",
      "{'input_ids': device(type='cpu'), 'attention_mask': device(type='cpu'), 'labels': device(type='cpu')}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "batch = next(iter(tokenized_dataset['train']))\n",
    "print({k: v.device for k, v in batch.items() if hasattr(v, 'device')})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ffaaee1-e93d-4845-b663-5abea21ef857",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 1 || lr: 5.014e-02 || reg: 7.788e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 5.014e-02||reg: 7.788e-08"
     ]
    }
   ],
   "source": [
    "log_lr = np.random.uniform(-10, 5)\n",
    "log_reg = np.random.uniform(-10, 5)\n",
    "lr = 10 ** log_lr\n",
    "reg = 10 ** log_reg\n",
    "\n",
    "# ‚úÖ log info in one line like your print version\n",
    "logger.info(f\"Run {1} || lr: {lr:.3e} || reg: {reg:.3e}\")\n",
    "# print(f\"Run {i+1}||\", end='')\n",
    "print(f\"lr: {lr:.3e}||\", end='')\n",
    "print(f\"reg: {reg:.3e}\", end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4dca657d-e6a8-42c0-aadf-2a3f69f55cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13632' max='13632' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13632/13632 2:14:38, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.372000</td>\n",
       "      <td>0.363314</td>\n",
       "      <td>0.531576</td>\n",
       "      <td>0.409221</td>\n",
       "      <td>0.462442</td>\n",
       "      <td>0.881093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.333900</td>\n",
       "      <td>0.344586</td>\n",
       "      <td>0.539962</td>\n",
       "      <td>0.450871</td>\n",
       "      <td>0.491411</td>\n",
       "      <td>0.885402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13632, training_loss=0.3793022414328347, metrics={'train_runtime': 8083.401, 'train_samples_per_second': 26.983, 'train_steps_per_second': 1.686, 'total_flos': 1.2816425860625376e+16, 'train_loss': 0.3793022414328347, 'epoch': 2.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_wnut_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83248e44-5c0a-43de-8f5f-c25645c7b71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['token', 'aspects', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d7e8e0-c5df-429b-8a92-15c344c17517",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (capstone)",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
